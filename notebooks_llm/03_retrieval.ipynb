{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import imodelsx.embeddings\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "df = pd.read_pickle(\"../data/data_clean.pkl\")\n",
    "n = df.shape[0]\n",
    "\n",
    "# set up text for prediction\n",
    "def get_text_representation(row):\n",
    "    # return f\"\"\"- Title: {row[\"title\"]}\n",
    "# - Description: {row[\"description\"]}\n",
    "# - Predictor variables: {str(row[\"feature_names\"])[1:-1]}\"\"\"\n",
    "    return f\"\"\"{row[\"title\"]}. {row[\"description\"]}.\"\"\"  # Keywords: {str(row[\"info___keywords\"])[1:-1]}\"\"\"\n",
    "    # return f\"\"\"{row[\"title\"]}. {row[\"description\"]}. Keywords: {str(row[\"info___keywords\"])[1:-1]}\"\"\"\n",
    "df['text'] = df.apply(get_text_representation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['description', 'info___usage___use_case', 'info___usage___why_use',\n",
    "    'info___usage___notes', 'info___next_steps___advice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_idx(id, df):\n",
    "    return np.where(df.id == id)[0]\n",
    "\n",
    "\n",
    "sims = np.zeros((n, n))\n",
    "for r, row in tqdm(df.iterrows()):\n",
    "    ids = row[\"info___related_cdi_ids\"]\n",
    "    for id in ids:\n",
    "        c = id_to_idx(id, df)\n",
    "        sims[r, c] += 1\n",
    "\n",
    "    # for c, col in df.iterrows():\n",
    "    #     for key in [\n",
    "    #         \"categorization___chief_complaint\",\n",
    "    #         \"categorization___specialty\",\n",
    "    #         \"categorization___purpose\",\n",
    "    #         \"categorization___system\",\n",
    "    #         \"categorization___disease\",\n",
    "    #     ]:\n",
    "    #         if row[key] == col[key]:\n",
    "    #             sims[r, c] += 1\n",
    "\n",
    "\n",
    "# average values across the diagonal\n",
    "sims = (sims + sims.T) / 2\n",
    "\n",
    "# set diagonal to 1\n",
    "# np.fill_diagonal(sims, max(sims))\n",
    "\n",
    "# plot clustermap\n",
    "# sns.clustermap(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = \"bert-base-uncased\"\n",
    "# checkpoint = 'microsoft/deberta-v2-xxlarge'\n",
    "checkpoint = 'tf-idf'\n",
    "embs = imodelsx.embeddings.get_embs(\n",
    "    df[\"text\"].tolist(), checkpoint, batch_size=32, aggregate=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs = normalize(embs, norm=\"l2\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median dist 0.71 versus baseline 0.99 (distances, so lower is better)\n",
      "mean rank (lower is better) 347.29 random baseline 344.5\n"
     ]
    }
   ],
   "source": [
    "# calculate pairwise similarity between all embeddings in embs\n",
    "s = pairwise_distances(embs, metric=\"cosine\")\n",
    "\n",
    "# matrix showing gt\n",
    "sims_sym = (sims >= 1.0).astype(bool)\n",
    "# sims_sym = np.where(sims >= 1.0)\n",
    "\n",
    "print('median dist', np.median(s[sims_sym]).round(2),\n",
    "      'versus baseline', np.median(s[~sims_sym]).round(2), '(distances, so lower is better)')\n",
    "\n",
    "\n",
    "# ranks for largest values in each row (0 is best rank)\n",
    "ranks_by_dist = np.argsort(s, axis=1)[::, ::-1]\n",
    "\n",
    "# select ranks for gt\n",
    "ranks_for_gt = []\n",
    "for i in range(sims_sym.shape[0]):\n",
    "    ranks_for_gt.append(ranks_by_dist[i, sims_sym[i]])\n",
    "\n",
    "# look at ranks\n",
    "all_ranks = np.concatenate(ranks_for_gt) - 1\n",
    "mean_rank = np.mean(all_ranks).round(2)\n",
    "print('mean rank (lower is better)', mean_rank,\n",
    "      'random baseline', (ranks_by_dist.shape[1] - 1) / 2)\n",
    "# plt.axvline(mean_rank, color=\"k\", linestyle=\"dashed\", linewidth=1)\n",
    "# plt.hist(all_ranks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
