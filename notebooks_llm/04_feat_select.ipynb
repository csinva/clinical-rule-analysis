{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import imodels\n",
    "import eval_feat_select\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "import dvu\n",
    "import imodelsx\n",
    "import imodelsx.viz\n",
    "import imodelsx.llm\n",
    "from pprint import pprint\n",
    "import feat_select\n",
    "import viz_feat_select\n",
    "\n",
    "import openai\n",
    "openai.api_key_path = '/home/chansingh/.OPENAI_KEY'\n",
    "dvu.set_style()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'tbi_young'\n",
    "if 'iai' in outcome:\n",
    "    X, y, feats_raw, feats_abbrev_unique = feat_select.get_iai_data(outcome)\n",
    "elif 'tbi' in outcome:\n",
    "    X, y, feats_raw, feats_abbrev_unique = feat_select.get_tbi_data(outcome)\n",
    "print(f\"Unique features: {len(feats_abbrev_unique)}\", 'X shape', X.shape)\n",
    "\n",
    "\n",
    "# # plt.figure(figsize=(8, 12))\n",
    "# # n = df_full.shape[1] - 1\n",
    "# # plt.barh(y=np.arange(n), width=df_full.corr()[\"outcome\"][:-1])\n",
    "# # plt.yticks(np.arange(n), pd.Series(df_full.columns[:-1]).apply(lambda x: x[:20]))\n",
    "# # plt.grid()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "# feats_select = rng.choice(list(feats_abbrev_unique), size=5, replace=False)\n",
    "# feats_select = ['AbdTrauma']\n",
    "feats_select = feat_select.FEATS[outcome]['pecarn_feats_ordered']\n",
    "idxs_raw = feat_select.abbrevs_to_idxs_raw(feats_select, feats_raw)\n",
    "\n",
    "mets = eval_feat_select.evaluate_features(\n",
    "    X[:, idxs_raw], y, seed=42, class_weight=2, return_pr_curve=True\n",
    ")\n",
    "# prec, rec, thresh = mets['roc_auc_curve']\n",
    "# plt.plot(rec, prec, '.')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mets_avg(strategies, feats_abbrev_unique, X, y):\n",
    "    mets_list_strategies = defaultdict(list)\n",
    "    for strategy in tqdm(strategies):\n",
    "        for seed in range(2):\n",
    "            mets_seed = defaultdict(list)\n",
    "            feats_ordered = feat_select.get_feats_ordered(\n",
    "                feats_abbrev_unique, strategy=strategy, seed=seed\n",
    "            )\n",
    "            for i in range(len(feats_ordered)):\n",
    "                feats_select = feats_ordered[: i + 1]\n",
    "                idxs_raw = feat_select.abbrevs_to_idxs_raw(feats_select, feats_raw)\n",
    "                met_scores = eval_feat_select.evaluate_features(\n",
    "                    X[:, idxs_raw], y, seed=42 + seed\n",
    "                )\n",
    "                for k in met_scores:\n",
    "                    mets_seed[k].append(met_scores[k])\n",
    "                mets_seed[\"n_feats\"].append(i + 1)\n",
    "            mets_list_strategies[strategy].append(pd.DataFrame(mets_seed))\n",
    "\n",
    "    # convert mets_list_strategies to mets_avg\n",
    "    mets_avg = defaultdict(list)\n",
    "    for strategy in strategies:\n",
    "        m = mets_list_strategies[strategy]\n",
    "        cols = m[0].columns\n",
    "        mets_mean = pd.DataFrame(\n",
    "            data=np.mean([m[i].values for i in range(len(m))], axis=0),\n",
    "            columns=cols,\n",
    "        )\n",
    "        mets_sem = pd.DataFrame(\n",
    "            data=np.std([m[i].values for i in range(len(m))], axis=0) / np.sqrt(len(m)),\n",
    "            columns=[k + \"_sem\" for k in cols],\n",
    "        )\n",
    "        mets_avg[strategy] = pd.concat([mets_mean, mets_sem], axis=1)\n",
    "\n",
    "    return mets_avg\n",
    "\n",
    "\n",
    "# mets_avg = compute_mets_avg([\"gpt-4-0314\", \"pecarn\", \"random\", \"pecarn___gpt-4-0314\"], feats_abbrev_unique, X, y)\n",
    "mets_avg = compute_mets_avg(\n",
    "    [\"gpt-4-0314\", \"random\"],\n",
    "    [\n",
    "        feat\n",
    "        for feat in feats_abbrev_unique\n",
    "        if not feat in feat_select.PECARN_FEATS_ORDERED_IAI\n",
    "    ],\n",
    "    X,\n",
    "    y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibly-memorized plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_feat_select.viz_curves(mets_avg, strategies=['pecarn', 'gpt-4-0314', 'random'], outcome=outcome, n_end=8)\n",
    "viz_feat_select.viz_curves(mets_avg, strategies=['gpt-4-0314', 'random'], outcome=outcome, n_end=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolating to new features plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_feat_select.viz_curves(mets_avg, strategies=['pecarn', 'pecarn___gpt-4-0314'], outcome=outcome, n_start=8-1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
