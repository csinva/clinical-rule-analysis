{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import dvu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import mdcalc\n",
    "import viz\n",
    "\n",
    "# plt.style.use('default')\n",
    "dvu.set_style()\n",
    "\n",
    "df = pd.read_pickle('../data/cdis_with_schemas_cleaned.pkl')\n",
    "\n",
    "def get_feature_names_list(schema):\n",
    "    if isinstance(schema, list):\n",
    "        return [mdcalc.clean_feature_name(s['label_en']) if 'label_en' in s else 'unknown'\n",
    "                for s in schema]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# these seem to be extra info in the calc, not actually a new feature\n",
    "def remove_unknown(x):\n",
    "    return [z for z in x if not z == 'unknown']\n",
    "\n",
    "df['feature_names'] = df['input_schema'].apply(get_feature_names_list)\n",
    "df['feature_names'] = df['feature_names'].apply(remove_unknown)\n",
    "df['feature_names_unique_uncleaned'] = df['feature_names'].apply(lambda l: list(set(l)))\n",
    "df['feature_names_unique'] = df['feature_names'].apply(\n",
    "    lambda l: list(set([mdcalc.rename_feature_name(x) for x in l])))\n",
    "df[\"feature_score_tuples_list\"] = df[\"input_schema\"].apply(\n",
    "    mdcalc.get_feature_score_tuples_list_from_schema\n",
    ")\n",
    "df['num_features_unique'] = df['feature_names_unique'].apply(len)\n",
    "\n",
    "# make plot\n",
    "all_words = sum(df['feature_names_unique'], [])  # concatenate all list\n",
    "all_words = pd.Series(all_words)\n",
    "# all_words = all_words[~(all_words == 'unknown')] # not necessary, already handleed\n",
    "counts = all_words.value_counts()\n",
    "feat_names = pd.Series(counts.index)\n",
    "\n",
    "\n",
    "counts.head(30)\n",
    "plt.figure(figsize=(9, 7.5), dpi=300, facecolor='w')\n",
    "N = 27\n",
    "# plt.grid()\n",
    "ax = sns.barplot(y=feat_names[:N].apply(lambda x: x[:39]),\n",
    "                 x=counts.values[:N], orient='h', color='mediumseagreen', width=0.9)\n",
    "# ax.bar_label(ax.containers[0], fontsize='small')\n",
    "# bar_label inside of bar\n",
    "# ax.bar_label(ax.containers[0], fontsize='small', labels=[str(x) + f' ({100 * x/df.shape[0]:.1f}%)' for x in counts.values[:N]])\n",
    "# add text to each bar\n",
    "# texts = [str(x) + f' ({100 * x/df.shape[0]:.1f}%)' for x in counts.values[:N]]\n",
    "texts = [str(x) for x in counts.values[:N]]\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.annotate(texts[i], (p.get_width(), p.get_y() + 0.75), ha='right', color='white')\n",
    "texts_perc = [f'{100 * x/df.shape[0]:.1f}%' for x in counts.values[:N]]\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.annotate(texts_perc[i], (p.get_width() + 2, p.get_y() + 0.72), ha='left', color='gray', fontsize='small')\n",
    "plt.xlabel(f'Number of CDIs that use this predictor variable\\n(Out of {df.shape[0]} CDIs)')\n",
    "plt.tight_layout()\n",
    "viz.savefig('common_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ref_text'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.ref_text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://pubmed.ncbi.nlm.nih.gov/33481930' in df_old['ref_href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, ids_with_paper = paper_setup.download_gsheet(fill_href=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' Adrogué HJ, Madias NE. Hyponatremia. NEJM, 2000.', ' Apgar V. A proposal for a new method of evaluation of the newborn infant. Curr. Res. Anesth. Analg. 1953;32(4): 260–267. doi:10.1213/00000539-195301000-00041. PMID 13083014.', ' David A. Morrow, et. al. TIMI Risk Score for ST-Elevation Myocardial Infarction: A Convenient, Bedside, Clinical Score for Risk Assessment at Presentation: An Intravenous nPA for Treatment of Infarcting Myocardium Early II Trial Substudy Circulation. 2000; 102: 2031-2037 doi: 10.1161/01.CIR.102.17.2031', ' Haimovich A, Ravindra NG, Stoytchev S, et al. Development and validation of the quick COVID-19 severity index (qCSI): a prognostic tool for early clinical decompensation. Annals of Emergency Medicine. 2020. ', ' Inouye SK, van Dyck CH, Alessi CA, Balkin S, Siegal AP, Horwitz RI. Clarifying confusion: the confusion assessment method. A new method for detection of delirium. Ann Intern Med. 1990;113(12):941–948.', \"A predictive model for aggressive non-Hodgkin's lymphoma. The International Non-Hodgkin's Lymphoma Prognostic Factors Project. N Engl J Med. 1993;329(14):987-94.\", 'AABB Technical Manual 18th Ed 2014.pdf. By Mark K. Fung, Brenda J. Grossman, Christopher D. Hillyer, and Connie Westhoff.', 'ARDSnet investigators.  Ventilation with lower tidal volumes as compared with traditional tidal volumes for acute lung injury and the acute respiratory distress syndrome.  N Engl J Med.  2000; May;342(18): 1301-1308.', 'Abbey J, et al. The Abbey pain scale: a 1-minute numerical indicator for people with end-stage dementia. Int J Palliat Nurs. 2004 Jan;10(1):6-13', 'Acker SN, Ross JT, Partrick DA, Tong S, Bensard DD. Pediatric specific shock index accurately identifies severely injured children. J Pediatr Surg. 2015;50(2):331-4.', 'Adrogue HJ and Madias NE. Primary Care: Hypernatremia. New England Journal of Medicine 2000; 342(20):1493-1499.', 'Adrogué HJ, Madias NE. Hyponatremia. N Engl J Med 2000; 342:1581.', 'Aerts M, Minalu G, Bösner S, et al. Pooled individual patient data from five countries were used to derive a clinical prediction rule for coronary artery disease in primary care. J Clin Epidemiol. 2017;81:120-128.', 'Al-Gwaiz LA, Babay HH. The diagnostic value of absolute neutrophil count, band count and morphologic changes of neutrophils in predicting bacterial infections. Med Princ Pract. 2007;16(5):344–7. doi:10.1159/000104806.', 'Albert MS, Dell RB, Winters RW. Quantitative Displacement of Acid-Base Equilibrium in Metabolic Acidosis. Ann Intern Med. 1967;66:312-322. doi:10.7326/0003-4819-66-2-312.', 'Aletaha D, Nell VP, Stamm T, et al. Acute phase reactants add little to composite disease activity indices for rheumatoid arthritis: validation of a clinical activity score. Arthritis Research & Therapy. 2005;7(4):R796-R806. doi:10.1186/ar1740.', 'Aletaha D, Neogi T, Silman AJ, et al. 2010 Rheumatoid arthritis classification criteria: an American College of Rheumatology/European League Against Rheumatism collaborative initiative. Arthritis Rheum. 2010;62(9):2569-81.', 'Allgöwer M, Burri C. The “shock-index”. Dtsch med Wochenschr 1967; 92(43): 1947-1950. DOI: 10.1055/s-0028-1106070', 'Almandoz JED, Schaefer PW, Goldstein JN, et al. Practical Scoring System for the Identification of Patients with Intracerebral Hemorrhage at Highest Risk of Harboring an Underlying Vascular Etiology: The Secondary Intracerebral Hemorrhage Score. AJNR American journal of neuroradiology. 2010;31(9):1653-1660. doi:10.3174/ajnr.A2156.', 'Alteplase package insert.', 'Altman DG, Bland JM. Diagnostic tests. 1: Sensitivity and specificity. BMJ. 1994;308(6943):1552.', 'Alvarado A. A practical score for the early diagnosis of acute appendicitis. Ann Emerg Med. 1986 May;15(5):557-64. PMID: 3963537.', 'Alvarez, F. et al. International Autoimmune Hepatitis Group Report: review of criteria for diagnosis of autoimmune hepatitis. Journal of Hepatology, Volume 31 , Issue 5 , 929 - 938.', 'Amarenco P, Bogousslavsky J, Caplan LR, Donnan GA, Wolf ME, Hennerici MG. The ASCOD phenotyping of ischemic stroke (Updated ASCO Phenotyping). Cerebrovasc Dis. 2013;36(1):1-5.', 'American Psychiatric Association: Desk Reference to the Diagnostic Criteria From DSM-5. Arlington, VA, American Psychiatric Association, 2013.', 'American Psychiatric Association: Desk Reference to the Diagnostic Criteria From DSM-5. Arlington, VA, American Psychiatric Association, 2013.', 'American Psychiatric Association: Desk Reference to the Diagnostic Criteria From DSM-5. Arlington, VA, American Psychiatric Association, 2013.', 'American Psychiatric Association: Desk Reference to the Diagnostic Criteria From DSM-5. Arlington, VA, American Psychiatric Association, 2013.', 'Ammirati F, Colivicchi F, Santini M. Diagnosing syncope in clinical practice. Implementation of a simplified diagnostic algorithm in a multicentre prospective trial - the OESIL 2 study (Osservatorio Epidemiologico della Sincope nel Lazio). Eur Heart J. 2000;21(11):935-40.']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(df_old[df_old[col].notna()][col])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paper_setup\n",
    "from tqdm import tqdm\n",
    "df_old = pd.read_pickle('../data/cdis_with_schemas_cleaned.pkl')\n",
    "\n",
    "assert df.shape[0] == df_old.shape[0]\n",
    "dic = {}    \n",
    "\n",
    "col = 'ref_text'\n",
    "for x in tqdm(sorted(df[col][df[col].notna()])):\n",
    "    print(x)\n",
    "    row_old = df_old[df_old[col].str.strip(' /') == x.strip(' /')].iloc[0]\n",
    "    row = df[df[col] == x].iloc[0]\n",
    "    if not row_old.id == row.id:\n",
    "        dic[x] = row_old.id, row.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df['num_splits'] = df['feature_names'].apply(len)\n",
    "\n",
    "plt.figure(dpi=300, facecolor='w', figsize=(6, 5))\n",
    "# plt.hist(df['num_splits'], label='Splits in CDI', bins=25)\n",
    "avg = np.median(df['num_features_unique'])\n",
    "plt.axvline(x=avg, ls='-', color='#444')\n",
    "plt.text(s=f'Median: {avg:0.0f}', x=avg + 1, y=175, color='#444')\n",
    "plt.hist(df['num_features_unique'], bins=25, color='mediumseagreen') #, alpha=0.8)\n",
    "plt.xlabel('Unique predictor variables in CDI')\n",
    "# plt.ylabel(f'Count of CDIs\\n(out of {df.shape[0]} CDIs)')\n",
    "plt.ylabel('CDI count')\n",
    "plt.title('B', loc='left', fontsize='x-large', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.legend()\n",
    "viz.savefig('num_rules_hist')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vals, counts = np.unique(df['num_splits'], return_counts=True)\n",
    "# plt.plot(vals, np.cumsum(counts) / np.sum(counts), '.-', label='Splits in CDI')\n",
    "# val_90 = (np.cumsum(counts) / np.sum(counts)) >= 0.9\n",
    "\n",
    "# plt.show()\n",
    "vals, counts = np.unique(df['num_features_unique'], return_counts=True)\n",
    "plt.plot(vals, np.cumsum(counts) / np.sum(counts), '.-', label='Unique features in CDI')\n",
    "\n",
    "plt.xlabel('Number of unique features')\n",
    "plt.ylabel('Fraction of CDIs with\\nat most this many splits')\n",
    "plt.legend()\n",
    "viz.savefig('num_rules_cdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep scoring-only CDIs\n",
    "feature_score_tuples_list = df[\"feature_score_tuples_list\"][\n",
    "    df[\"feature_score_tuples_list\"].apply(len) > 0\n",
    "]\n",
    "print(\"only keeping\", feature_score_tuples_list.shape[0], \"CDIs out of\", df.shape[0])\n",
    "\n",
    "# get score_dict\n",
    "score_tuples = sorted(sum(feature_score_tuples_list, []), key=lambda x: x[0])\n",
    "ks = sorted(set([score_tuple[0] for score_tuple in score_tuples]))\n",
    "score_dict = defaultdict(list)\n",
    "for key, score in score_tuples:\n",
    "    score_dict[key].append(score)\n",
    "\n",
    "# filter list\n",
    "scores_list = [\n",
    "    (k, v)\n",
    "    for (k, v) in score_dict.items()\n",
    "    if len(v) >= 9\n",
    "    # or k == 'Race/Ethnicity'\n",
    "]\n",
    "# scores_list = sorted(score_dict_filt, key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# format into df\n",
    "feat_name_vec = sum([[k] * len(v) for (k, v) in scores_list], [])\n",
    "score_vec = sum([v for (k, v) in scores_list], [])\n",
    "df_box = pd.DataFrame.from_dict({\"feat_name\": feat_name_vec, \"score\": score_vec})\n",
    "medians = df_box.groupby([\"feat_name\"])[\"score\"].median().sort_values(ascending=False)\n",
    "\n",
    "# plot\n",
    "plt.figure(dpi=300, figsize=(7, 4))\n",
    "# place grid in background behind other elements\n",
    "sns.boxplot(\n",
    "    x=\"score\",\n",
    "    y=\"feat_name\",\n",
    "    data=df_box,\n",
    "    color=\"mediumseagreen\",\n",
    "    order=medians.index,\n",
    "    # fliersize=3.1,\n",
    "    whis=1.5,\n",
    "    showfliers=False,\n",
    "\n",
    "    # change color of fliers\n",
    "    # flierprops=dict(alpha=1, marker='o', color='#EEE'),\n",
    ")\n",
    "plt.ylabel(\"Predictor variable\")\n",
    "plt.yticks(fontsize=\"small\")\n",
    "plt.xticks(fontsize=\"small\")\n",
    "plt.xlabel(\n",
    "    \"Variable importance (fraction of total points)\",\n",
    "    fontsize=\"small\",\n",
    ")\n",
    "# plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "viz.savefig(\"scoring_cdi_boxplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter CDIs with potentially biased features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fam_hist(x):\n",
    "    return [s for s in x if \"fam\" in s.lower() and \"history\" in s.lower()]\n",
    "\n",
    "\n",
    "df[\"fam_hist_var\"] = df[\"feature_names_unique_uncleaned\"].apply(check_fam_hist)\n",
    "fam_hist = df[\"fam_hist_var\"].apply(len) > 0\n",
    "df['fam_hist_var'] = df['fam_hist_var'].apply(lambda x: str(x)[1: -1].replace(\"'\", \"\"))\n",
    "print(fam_hist.sum(), \"family history CDIs\")\n",
    "d = df[[\"full_title_en\", \"short_description_en\", \"ref_text\", \"fam_hist_var\"]][fam_hist].sort_index()\n",
    "d.to_csv(\"../results/fam_hist_cdis.csv\", index=False)\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_race_ethnicity(x):\n",
    "    return [s for s in x if mdcalc.rename_feature_name(s) == 'Race/Ethnicity']\n",
    "\n",
    "df[\"race_var\"] = df[\"feature_names_unique_uncleaned\"].apply(check_race_ethnicity)\n",
    "race = df[\"race_var\"].apply(len) > 0\n",
    "df['race_var'] = df['race_var'].apply(lambda x: str(x)[1: -1].replace(\"'\", \"\"))\n",
    "print(race.sum(), \"race CDIs\")\n",
    "d = df[[\"full_title_en\", \"short_description_en\", \"ref_text\", \"race_var\"]][race].sort_index()\n",
    "d.to_csv(\"../results/race_cdis.csv\", index=False)\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output feature rename table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Look at the renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feats_print = sorted(set(sum(df['feature_names'], [])))\n",
    "df_print = pd.DataFrame(feats_print, columns=['Original feature'])\n",
    "df_print['Chandan rename'] = df_print['Original feature'].apply(mdcalc.rename_feature_name)\n",
    "idxs_same = df_print['Chandan rename'] == df_print['Original feature']\n",
    "df_print['Chandan rename'][idxs_same] = ''\n",
    "df_print.to_csv('../data/renaming/features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdcalc import (\n",
    "    KEYWORDS_CONTAIN,\n",
    "    KEYWORDS_CASED_CONTAIN,\n",
    "    KEYWORD_PREFIXES,\n",
    "    KEYWORDS_MAP,\n",
    "    KEYWORD_PREFIXES_CASED_MAP,\n",
    "    KEYWORD_RENAME_FINAL_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"KEYWORDS_CONTAIN\": KEYWORDS_CONTAIN,\n",
    "    \"KEYWORDS_CASED_CONTAIN\": KEYWORDS_CASED_CONTAIN,\n",
    "    \"KEYWORD_PREFIXES\": KEYWORD_PREFIXES,\n",
    "    \"KEYWORDS_MAP\": [f\"{k} → {v}\" for k, v in KEYWORDS_MAP.items()],\n",
    "    \"KEYWORD_PREFIXES_CASED_MAP\": [f\"{k} → {v}\" for k, v in KEYWORD_PREFIXES_CASED_MAP.items()],\n",
    "    \"KEYWORD_RENAME_FINAL\": [f\"{k} → {v}\" for k, v in KEYWORD_RENAME_FINAL_MAP.items()],\n",
    "}\n",
    "n = max(len(v) for v in d.values())\n",
    "r = pd.DataFrame()\n",
    "for k, v in d.items():\n",
    "    r[k] = pd.Series(v)\n",
    "r.to_csv(\"../data/renaming/keywords_export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example debugging schema\n",
    "x = df[df.slug == 'cha2ds2-vasc-score-atrial-fibrillation-stroke-risk'].iloc[0]\n",
    "schema = x['input_schema']\n",
    "schema\n",
    "for s in schema:\n",
    "    options = s['options']\n",
    "    print(s['name'], mdcalc.clean_feature_name(s['label_en']), [opt['value'] for opt in options])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", 1000):\n",
    "    # display(df[df[\"feature_score_tuples_list\"].apply(len) == 2].iloc[3][['url_full', 'input_schema']])\n",
    "    display(df[df[\"feature_score_tuples_list\"].apply(len) == 2][['url_full', 'feature_score_tuples_list']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", 1000):\n",
    "    slug = 'prognostic-index-cancer-outcomes'\n",
    "    display(\n",
    "        df[df.slug == slug][\n",
    "            [\"url_full\", \"input_schema\", \"feature_score_tuples_list\"]\n",
    "        ].iloc[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full\n",
    "with pd.option_context(\"display.max_colwidth\", 1000):\n",
    "    # display(\n",
    "    #     df.sort_values(by=\"num_features_unique\", ascending=False)[\n",
    "    #         [\"url_full\", \"feature_names_unique\", \"slug\", \"input_schema\"]\n",
    "    #     ]\n",
    "    # )\n",
    "    display(\n",
    "        df[df[\"feature_score_tuples_list\"].apply(len) == 2][\n",
    "            [\"url_full\", \"feature_names_unique\", \"slug\", \"input_schema\"]\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
