{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import dvu\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import os.path\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import imodelsx.llm\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import openai\n",
    "import prompts\n",
    "openai.api_key = open('/home/chansingh/.OPENAI_KEY').read().strip()\n",
    "plt.style.use('default')\n",
    "dvu.set_style()\n",
    "\n",
    "df = pd.read_csv('../data/main.csv')\n",
    "df = df[df['id'].notna()]\n",
    "\n",
    "# extract text from pdfs\n",
    "ids_with_paper = df[\n",
    "    (df[\"found_paper (0=no, 1=yes)\"] == 1)\n",
    "    | (df[\"found_paper (0=no, 1=yes)\"] == 2)\n",
    "].id\n",
    "\n",
    "# d = pd.read_csv('../data/mini.csv')\n",
    "def extract_texts_from_pdf(ids, papers_dir='../papers'):\n",
    "    for id in tqdm(ids):\n",
    "        paper_file = join(papers_dir, str(id) + \".pdf\")\n",
    "        if pathlib.Path(paper_file).exists():\n",
    "            with fitz.open(paper_file) as doc:  # open document\n",
    "                text = chr(12).join([page.get_text() for page in doc])\n",
    "                text = text.replace('-\\n', '')\n",
    "                pathlib.Path(join(papers_dir, str(id) + \".txt\")).write_bytes(\n",
    "                    text.encode()\n",
    "                )\n",
    "\n",
    "extract_texts_from_pdf(ids_with_paper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask questions about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = imodelsx.llm.get_llm(\"gpt-3.5-turbo-0613\")\n",
    "llm = imodelsx.llm.get_llm(\"gpt-4-0613\")\n",
    "# llm = imodelsx.llm.get_llm(\"gpt-4-32k-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties, functions, content_str = prompts.get_prompts_demographics()\n",
    "properties, functions, content_str = prompts.get_prompts_gender()\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": content_str,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with answer: One hundred and five patients, 55 males and 50 females\n",
    "toy_input1 = \"\"\"This study was about treating diabetes. It was a very difficult study.\n",
    "One hundred and five patients, 55 males and 50 females were included.\n",
    "The study took 200 days to complete. The study was conducted in the United States.\n",
    "The study was conducted by the University of California, San Francisco.\"\"\"\n",
    "\n",
    "# example with answer: One hundred and five patients, 55 males and 50 females, 10 white, 75 black\n",
    "toy_input2 = \"\"\"This study was about treating diabetes. It was a very difficult study.\n",
    "One hundred and five patients, 55 males and 50 females were included.\n",
    "The study took 200 days to complete. The study was conducted in the United States.\n",
    "Ten of the patients were white, 20 were asian, and the rest were black.\n",
    "The study was conducted by the University of California, San Francisco.\"\"\"\n",
    "\n",
    "# messages[0]['content'] = content_str.format(input=toy_input1)\n",
    "# msg = llm(messages, functions=functions, return_str=False, temperature=0.0)\n",
    "# args = json.loads(msg.get('function_call')['arguments'])\n",
    "# print(json.dumps(args, indent=2))\n",
    "\n",
    "# messages[0]['content'] = content_str.format(input=toy_input2)\n",
    "# msg = llm(messages, functions=functions, return_str=False, temperature=0.0)\n",
    "# args = json.loads(msg.get('function_call')['arguments'])\n",
    "# print(json.dumps(args, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_to_none(x: str):\n",
    "    if x in {\"\", \"unknown\", \"N/A\"}:\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def call_on_subsets(x: str, subset_len_tokens=4750, max_calls=3):\n",
    "    subset_len_chars = subset_len_tokens * 4\n",
    "\n",
    "    args = None\n",
    "    subset_num = 0\n",
    "\n",
    "    while args is None and subset_num < max_calls:\n",
    "        subset = x[subset_num * subset_len_chars : (subset_num + 1) * subset_len_chars]\n",
    "\n",
    "        # if approx_tokens < 6000:\n",
    "        messages[0][\"content\"] = content_str.format(input=subset)\n",
    "        msg = llm(messages, functions=functions, return_str=False, temperature=0.0)\n",
    "        if msg is not None and msg.get(\"function_call\") is not None:\n",
    "            args = json.loads(msg.get(\"function_call\")[\"arguments\"])\n",
    "            return args\n",
    "\n",
    "        subset_num += 1\n",
    "\n",
    "        # next segment should have atleast 0.5 * subset_len_chars_left\n",
    "        if len(x) < (subset_num + 0.5) * subset_len_chars:\n",
    "            break\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_evidence(ev: str, real_input: str):\n",
    "    if ev is not None:\n",
    "        # remove all whitespace\n",
    "        ev = \"\".join(ev.split())\n",
    "        real_input = \"\".join(real_input.split())\n",
    "        return ev.lower() in real_input.lower()\n",
    "    return False\n",
    "\n",
    "\n",
    "# initialize\n",
    "for k in properties.keys():\n",
    "    df.loc[:, k] = None\n",
    "# df[\"approx_tokens\"] = None\n",
    "\n",
    "# run loop\n",
    "for id in tqdm(ids_with_paper):\n",
    "    i = df[df.id == id].index[0]\n",
    "    row = df.iloc[i]\n",
    "    paper_file = join(\"../papers\", str(int(row.id)) + \".txt\")\n",
    "    real_input = pathlib.Path(paper_file).read_text()\n",
    "    # gpt4 has 8k token window (some of it is functions, etc.)\n",
    "    # approx_tokens = (len(real_input) / 4)\n",
    "    # df.loc[i, \"approx_tokens\"] = approx_tokens\n",
    "\n",
    "    args = call_on_subsets(real_input)\n",
    "\n",
    "    # print(json.dumps(args, indent=2))\n",
    "    if args is not None:\n",
    "        for k in properties.keys():\n",
    "            if k in args:\n",
    "                # set the value at row number i and column k to the value of args[k]\n",
    "                df.loc[i, k] = rename_to_none(args[k])\n",
    "\n",
    "                # remove spans if they are not actually contained in the text\n",
    "                if k in [\"num_male_evidence_span\", \"num_female_evidence_span\"]:\n",
    "                    if not check_evidence(args[k], real_input):\n",
    "                        df.loc[i, k] = None\n",
    "print(\"completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "for k in ['num_male', 'num_female']:\n",
    "    idxs = (df[k + '_corrected'].notnull() & ~(df[k + '_corrected'] == 'Unk'))\n",
    "    gt = df[k + '_corrected'][idxs].astype(int)\n",
    "    pred = df[k].apply(cast_int)[idxs].astype(int)\n",
    "    acc = (gt == pred).mean()\n",
    "    print(f'{k} acc={acc:0.2f} n={len(gt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/main.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at gender ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = (df['num_male_corrected'].notnull() & ~(df['num_male_corrected'] == 'Unk')) & (df['num_female_corrected'].notnull() & ~(df['num_female_corrected'] == 'Unk'))\n",
    "male = df['num_male_corrected'][idxs].astype(int)\n",
    "female = df['num_female_corrected'][idxs].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = (male / female).values\n",
    "# drop inf\n",
    "ratios = ratios[~np.isinf(ratios)]\n",
    "sorted(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
