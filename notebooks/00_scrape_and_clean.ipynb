{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Last scraped on Jun 13, 2023. Output of this notebook is `../data/cdis_with_schemas_cleaned.pkl` (intermediate output is `../data/cdis_with_schemas.pkl`). Also outputs `../data/main.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read index\n",
    "# url='https://www.mdcalc.com/#All'\n",
    "# req=requests.get(url)\n",
    "# content=req.text\n",
    "# with open('../data/index.html', 'w') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# extract out df\n",
    "index = open('../data/index.html', 'r').read()\n",
    "soup = BeautifulSoup(index)\n",
    "tab = pd.read_table('../data/index.html')\n",
    "scripts = soup.find_all('script')\n",
    "d = json.loads(scripts[-1].text)\n",
    "all_calcs_list = d['props']['pageProps']['allCalcs']\n",
    "df = pd.DataFrame.from_dict(all_calcs_list)\n",
    "\n",
    "# clean df\n",
    "def clean_element(x):\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 1:\n",
    "            return x[0]\n",
    "    if isinstance(x, str):\n",
    "        return x.replace('<span>', '').replace('</span>', '')\n",
    "    return x\n",
    "df = df.applymap(clean_element)\n",
    "df['url_full'] = 'https://www.mdcalc.com/calc/' + df['id'].astype(str) + '/' + df['slug']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read individual pages\n",
    "Note: this actually does the scraping and saves `.html` files. Don't run multiple times (unless cached files are present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(df.shape[0])): #df.shape[0])):\n",
    "    slug = df['slug'].iloc[i]\n",
    "    url_full = df['url_full'].iloc[i]\n",
    "    output_fname = f'../data/pages/{slug}.html'\n",
    "\n",
    "    if not os.path.exists(output_fname):\n",
    "        req = requests.get(url_full)\n",
    "        content = req.text\n",
    "        with open(output_fname, 'w') as f:\n",
    "            f.write(content)\n",
    "            print(slug, url_full)\n",
    "assert len(os.listdir('../data/pages')) == df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Parse individual pages\n",
    "`.html` -> `.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    slug = df.iloc[i].slug\n",
    "    fname = '../data/pages/' + slug + '.html'\n",
    "    fname_json = '../data/processed/' + slug + '.json'\n",
    "    if not os.path.exists(fname_json):\n",
    "        html = open(fname, 'r').read()\n",
    "        soup = BeautifulSoup(html)\n",
    "        try:\n",
    "            data = soup.find_all('script')[2]\n",
    "            s = json.loads(data.text[data.text.index('{'):])['calc']\n",
    "            with open(f'../data/processed/{slug}.json', 'w') as f:\n",
    "                json.dump(s, f)\n",
    "        except:\n",
    "            # print('failed', slug)\n",
    "            errors.append(slug)\n",
    "print('num errors', len(errors))\n",
    "df = df[df.slug.isin(errors) == False]\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge metadata from index and individual pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cdis\n",
    "cdis = [\n",
    "    json.loads(open(f\"../data/processed/{df.iloc[i].slug}.json\", \"r\").read())\n",
    "    for i in tqdm(range(df.shape[0]))\n",
    "]\n",
    "cdis = pd.DataFrame.from_dict(cdis)\n",
    "# cdis['num_rules'] = cdis['input_schema'].apply(len)  # num rules (this includes some messiness that isn't actually a rule)\n",
    "\n",
    "# merge with df\n",
    "df_merged = df.join(cdis, rsuffix=\"_duplicate\")  # mark duplicate cols\n",
    "df_merged = df_merged.drop(\n",
    "    columns=[k for k in df_merged.columns if k.endswith(\"_duplicate\")]\n",
    ")  # drop the duplicates\n",
    "\n",
    "df_merged.to_pickle(\"../data/cdis_with_schemas.pkl\")\n",
    "df_merged.to_csv(\"../data/cdis_with_schemas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the cdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import mdcalc\n",
    "from mdcalc import try_or_none\n",
    "df = pd.read_pickle(\"../data/cdis_with_schemas.pkl\")\n",
    "df = df.sort_values(by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@try_or_none\n",
    "def get_refs(row):\n",
    "    return row[\"content\"][\"about\"][\"references_list\"]\n",
    "\n",
    "@try_or_none\n",
    "def get_ref_original(row):\n",
    "    return row['Original/Primary Reference']\n",
    "\n",
    "@try_or_none\n",
    "def get_text(row):\n",
    "    return row[0]['text']\n",
    "\n",
    "@try_or_none\n",
    "def get_href(row):\n",
    "    return row[0]['href']\n",
    "\n",
    "@try_or_none\n",
    "def get_year_from_str(s: str):\n",
    "    # search for a 4 digit number that between 1900 and 2023\n",
    "\n",
    "    match = re.search(r'(?<!\\d)(19\\d{2}|20[01]\\d|202[0-3])(?!\\d)', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean list-valued columns\n",
    "df = mdcalc.clean_list_valued_strings(df)\n",
    "\n",
    "# add feature_names\n",
    "def get_feature_names_list(schema):\n",
    "    if isinstance(schema, list):\n",
    "        return [s[\"label_en\"] if \"label_en\" in s else \"unknown\" for s in schema]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "df[\"feature_names\"] = df[\"input_schema\"].apply(get_feature_names_list)\n",
    "df[\"refs\"] = df.apply(get_refs, axis=1)\n",
    "df['ref_original'] = df['refs'].apply(get_ref_original)\n",
    "df['ref_href'] = df['ref_original'].apply(get_href)\n",
    "df['ref_text'] = df['ref_original'].apply(get_text)\n",
    "df['ref_year'] = df['ref_text'].apply(get_year_from_str)\n",
    "\n",
    "df.to_pickle(\"../data/cdis_with_schemas_cleaned.pkl\")\n",
    "df.to_csv(\"../data/cdis_with_schemas_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columnswith no max_width\n",
    "cols = ['id', 'full_title_en', 'short_description_en', 'ref_text', 'ref_href', 'ref_year']\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 0): \n",
    "    display(df[cols].head(5))\n",
    "    # display(df[df['ref_year'].min() == df['ref_year']][['ref_text', 'ref_href']])\n",
    "df[cols].to_csv('../data/main.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
